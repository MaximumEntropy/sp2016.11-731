{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import sys\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import math\n",
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use(\"gpu0\")\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('/usr1/home/ssandeep/IncrementalMT/SanDeepLearn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr1/home/ssandeep/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from recurrent import LSTM, FastLSTM\n",
    "from layer import FullyConnectedLayer, EmbeddingLayer\n",
    "from optimizers import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_train_src = 'data/train.src'\n",
    "path_to_train_tgt = 'data/train.tgt'\n",
    "path_to_dev_src = 'data/dev.src'\n",
    "path_to_dev_tgt = 'data/dev.tgt'\n",
    "path_to_test_src = 'data/test.src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_src = [line.strip().split() for line in codecs.open(path_to_train_src, 'r', encoding='utf8')]\n",
    "train_tgt = [line.strip().split() for line in codecs.open(path_to_train_tgt, 'r', encoding='utf8')]\n",
    "dev_src = [line.strip().split() for line in codecs.open(path_to_dev_src, 'r', encoding='utf8')]\n",
    "dev_tgt = [line.strip().split() for line in codecs.open(path_to_dev_tgt, 'r', encoding='utf8')]\n",
    "test_src = [line.strip().split() for line in codecs.open(path_to_test_src, 'r', encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_vocab = set()\n",
    "for line in train_src:\n",
    "    for word in line:\n",
    "        src_vocab.add(word)\n",
    "\n",
    "src_word2ind = {}\n",
    "src_ind2word = {}\n",
    "\n",
    "for ind, word in enumerate(src_vocab):\n",
    "    src_word2ind[word] = ind\n",
    "    src_ind2word[ind] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_vocab = set()\n",
    "for line in train_tgt:\n",
    "    for word in line:\n",
    "        target_vocab.add(word)\n",
    "\n",
    "target_word2ind = {}\n",
    "target_ind2word = {}\n",
    "\n",
    "for ind, word in enumerate(target_vocab):\n",
    "    target_word2ind[word] = ind\n",
    "    target_ind2word[ind] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_inp = T.ivector()\n",
    "tgt_inp = T.ivector()\n",
    "tgt_op = T.ivector()\n",
    "index = T.scalar()\n",
    "#src_lengths = T.ivector()\n",
    "#tgt_mask = T.fmatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_inp_t = np.random.rand(5,).astype(np.int32)\n",
    "tgt_inp_t = np.random.rand(5,).astype(np.int32)\n",
    "tgt_op_t = np.random.rand(5,).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Model\n",
    "#\n",
    "src_emb_dim      = 256  # source word embedding dimension\n",
    "tgt_emb_dim      = 256  # target word embedding dimension\n",
    "src_lstm_op_dim = 512  # source LSTMs hidden dimension\n",
    "tgt_lstm_op_dim = 2 * src_lstm_op_dim  # target LSTM hidden dimension\n",
    "beta = 500 # Regularization coefficient\n",
    "\n",
    "\n",
    "n_src = len(src_word2ind)  # number of words in the source language\n",
    "n_tgt = len(target_word2ind)  # number of words in the target language\n",
    "\n",
    "# Embedding Lookup Tables\n",
    "src_embedding_layer = EmbeddingLayer(input_dim=n_src, output_dim=src_emb_dim, name='src_embedding')\n",
    "tgt_embedding_layer = EmbeddingLayer(input_dim=n_tgt, output_dim=tgt_emb_dim, name='tgt_embedding')\n",
    "\n",
    "# Encoder BiLSTM and Decoder LSTM\n",
    "src_lstm_forward = LSTM(input_dim=src_emb_dim, output_dim=src_lstm_op_dim)\n",
    "src_lstm_backward = LSTM(input_dim=tgt_emb_dim, output_dim=src_lstm_op_dim)\n",
    "tgt_lstm = LSTM(input_dim=tgt_emb_dim, output_dim=tgt_lstm_op_dim)\n",
    "\n",
    "# Projection layers\n",
    "proj_layer1 = FullyConnectedLayer(input_dim=tgt_lstm_op_dim + 2 * src_lstm_op_dim, output_dim=n_tgt, activation='softmax')\n",
    "proj_layer2 = FullyConnectedLayer(input_dim=2 * src_lstm_op_dim, output_dim=tgt_emb_dim, activation='tanh')\n",
    "\n",
    "params = src_embedding_layer.params + tgt_embedding_layer.params + src_lstm_forward.params + src_lstm_backward.params + tgt_lstm.params[:-1] + proj_layer1.params # + proj_layer2.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 512 1024 1024\n"
     ]
    }
   ],
   "source": [
    "print src_lstm_forward.input_dim, src_lstm_forward.output_dim, tgt_lstm.output_dim, tgt_lstm.output_dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source embedding (5, 256)\n",
      "target embedding (5, 256)\n",
      "src lstm forward (5, 512)\n",
      "src lstm backward (5, 512)\n",
      "bilstm (5, 1024)\n",
      "tgt lstm (5, 1024)\n",
      "attention vectors (5, 1024)\n",
      "taget representaiton (5, 2048)\n",
      "proj rep (5, 3121)\n",
      "cost 8.5069694519\n"
     ]
    }
   ],
   "source": [
    "#Get embedding matrices\n",
    "src_emb_inp = src_embedding_layer.fprop(src_inp)\n",
    "print 'source embedding', src_emb_inp.eval({src_inp:src_inp_t}).shape\n",
    "tgt_emb_inp = tgt_embedding_layer.fprop(tgt_inp)\n",
    "print 'target embedding', tgt_emb_inp.eval({tgt_inp:tgt_inp_t}).shape\n",
    "\n",
    "# Get BiLSTM representations\n",
    "src_lstm_forward.fprop(src_emb_inp)\n",
    "src_lstm_backward.fprop(src_emb_inp[::-1, :])\n",
    "encoder_representation = T.concatenate((src_lstm_forward.h, src_lstm_backward.h[::-1, :]), axis=1)\n",
    "print 'src lstm forward', src_lstm_forward.h.eval({src_inp:src_inp_t}).shape\n",
    "print 'src lstm backward', src_lstm_backward.h.eval({src_inp:src_inp_t}).shape\n",
    "print 'bilstm', encoder_representation.eval({src_inp:src_inp_t}).shape\n",
    "\n",
    "# Get Target LSTM representation & Attention Vectors\n",
    "tgt_lstm.h_0 = encoder_representation[-1]\n",
    "tgt_lstm.fprop(tgt_emb_inp)\n",
    "#repeated_src_context = T.repeat(encoder_representation[-1].dimshuffle('x', 0), tgt_emb_inp.shape[0], axis=0)\n",
    "#repeated_src_context = proj_layer2.fprop(repeated_src_context)\n",
    "#print 'repeated src_context', repeated_src_context.eval({src_inp: src_inp_t, tgt_inp: tgt_inp_t}).shape\n",
    "#tgt_sentence_emb = T.concatenate((tgt_emb_inp, repeated_src_context), axis=1)\n",
    "\n",
    "# Attention\n",
    "attention = tgt_lstm.h.dot(encoder_representation.transpose())\n",
    "attention = attention.dot(encoder_representation)\n",
    "print 'tgt lstm', tgt_lstm.h.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\n",
    "print 'attention vectors', attention.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\n",
    "\n",
    "# Concatenate the attention vectors to the Target LSTM output before predicting the next word\n",
    "target_representation = T.concatenate([attention, tgt_lstm.h], axis=1)\n",
    "print 'taget representaiton', target_representation.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\n",
    "\n",
    "# Predict the output sequence of words\n",
    "proj_output_rep = proj_layer1.fprop(target_representation)\n",
    "print 'proj rep', proj_output_rep.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\n",
    "\n",
    "# Compute cost\n",
    "cost = T.nnet.categorical_crossentropy(proj_output_rep, tgt_op).mean()\n",
    "cost += beta * T.mean((tgt_lstm.h[:-1] ** 2 - tgt_lstm.h[1:] ** 2) ** 2) # Regularization of RNNs from http://arxiv.org/pdf/1511.08400v6.pdf\n",
    "print 'cost', cost.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t, tgt_op:tgt_op_t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates=Optimizer(clip=5.0).adam(\n",
    "    cost=cost,\n",
    "    params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_train = theano.function(\n",
    "    inputs=[src_inp, tgt_inp, tgt_op],\n",
    "    outputs=cost,\n",
    "    updates=updates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_eval = theano.function(\n",
    "    inputs=[src_inp, tgt_inp],\n",
    "    outputs=proj_output_rep,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(src_sents, tgt_sents, valid=False):\n",
    "    assert len(src_sents) == len(tgt_sents)\n",
    "    src_lengths = [len(sent) for sent in src_sents]\n",
    "    src_max_len = max(src_lengths)\n",
    "    if valid == False:\n",
    "        tgt_lengths = [len(sent) for sent in tgt_sents]\n",
    "        tgt_max_len = max(tgt_lengths)\n",
    "    return (\n",
    "        np.array([[src_word2ind[x] for x in sent] + ([0] * (src_max_len - len(sent))) for sent in src_sents]).astype(np.int32),\n",
    "        np.array(src_lengths).astype(np.int32),\n",
    "        np.array([[target_word2ind[x] for x in sent[:-1]] + ([0] * (tgt_max_len - len(sent))) for sent in tgt_sents]).astype(np.int32),\n",
    "        np.array([[target_word2ind[x] for x in sent[1:]] + ([0] * (tgt_max_len - len(sent))) for sent in tgt_sents]).astype(np.int32),\n",
    "        [([1] * (l - 1)) + ([0] * (tgt_max_len - l)) for l in tgt_lengths]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_validation_predictions():\n",
    "    validation_predictions = []    \n",
    "    for ind, sent in enumerate(dev_src):\n",
    "        \n",
    "        if ind % 300 == 0:\n",
    "            print ind, len(dev_src)\n",
    "        src_words = np.array([src_word2ind[x] for x in sent]).astype(np.int32)\n",
    "        current_outputs = [target_word2ind['<s>']]\n",
    "\n",
    "        while True:\n",
    "            next_word = f_eval(src_words, current_outputs).argmax(axis=1)[-1]\n",
    "            current_outputs.append(next_word)\n",
    "            #print [target_ind2word[x] for x in current_outputs]\n",
    "            if next_word == target_word2ind['</s>'] or len(current_outputs) >= 15:\n",
    "                validation_predictions.append([target_ind2word[x] for x in current_outputs])\n",
    "                break\n",
    "    return validation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_predictions():\n",
    "    test_predictions = []    \n",
    "    for ind, sent in enumerate(test_src):\n",
    "        \n",
    "        if ind % 300 == 0:\n",
    "            print ind, len(test_src)\n",
    "        src_words = np.array([src_word2ind[x] for x in sent]).astype(np.int32)\n",
    "        current_outputs = [target_word2ind['<s>']]\n",
    "\n",
    "        while True:\n",
    "            next_word = f_eval(src_words, current_outputs).argmax(axis=1)[-1]\n",
    "            current_outputs.append(next_word)\n",
    "            #print [target_ind2word[x] for x in current_outputs]\n",
    "            if next_word == target_word2ind['</s>'] or len(current_outputs) >= 15:\n",
    "                test_predictions.append([target_ind2word[x] for x in current_outputs])\n",
    "                break\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 506\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-68e430e37fd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_test_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-187fcbe27c64>\u001b[0m in \u001b[0;36mget_test_predictions\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mnext_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mcurrent_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;31m#print [target_ind2word[x] for x in current_outputs]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr1/home/ssandeep/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr1/home/ssandeep/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr1/home/ssandeep/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_preds = get_test_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1006\n",
      "300 1006\n",
      "600 1006\n",
      "900 1006\n"
     ]
    }
   ],
   "source": [
    "validation_preds = get_validation_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'27.43'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_validation_bleu(valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "0 0.319494\n",
      "300"
     ]
    }
   ],
   "source": [
    "f = open('blue_valid_log.txt', 'w')\n",
    "all_costs = []\n",
    "batch_size = 50\n",
    "n_epochs = 100\n",
    "best_valid_preds = None\n",
    "best_valid_score = -sys.maxint\n",
    "best_test_preds = None\n",
    "for i in xrange(n_epochs):\n",
    "    print 'Starting epoch %i' % i\n",
    "    indices = range(len(train_src))\n",
    "    np.random.shuffle(indices)\n",
    "    train_src_batch = [train_src[ind] for ind in indices]\n",
    "    train_tgt_batch = [train_tgt[ind] for ind in indices]\n",
    "    assert len(train_src_batch) == len(train_tgt_batch)\n",
    "    costs = []\n",
    "    for j in xrange(len(train_src_batch)):\n",
    "        #s_sent, s_length, t_inp, t_op, mask = get_batch(train_src_batch[j:j + batch_size], train_tgt_batch[j:j+batch_size])\n",
    "        new_cost = f_train(\n",
    "            np.array([src_word2ind[x] for x in train_src_batch[j]]).astype(np.int32),\n",
    "            np.array([target_word2ind[x] for x in train_tgt_batch[j]][:-1]).astype(np.int32),\n",
    "            np.array([target_word2ind[x] for x in train_tgt_batch[j]][1:]).astype(np.int32),\n",
    "        )\n",
    "        all_costs.append((j, new_cost))\n",
    "        costs.append(new_cost)\n",
    "        if j % 300 == 0:\n",
    "            print j, np.mean(costs)\n",
    "            costs = []\n",
    "        if np.isnan(new_cost):\n",
    "            print 'NaN detected.'\n",
    "            break\n",
    "        if j % 10000 == 0 and j != 0:\n",
    "            valid_preds = get_validation_predictions()\n",
    "            print '==================================================================='\n",
    "            print 'Epoch %i BLEU on Validation : %s ' % (i, get_validation_bleu(valid_preds))\n",
    "            print '==================================================================='\n",
    "            if float(get_validation_bleu(valid_preds)) >= best_valid_score:\n",
    "                best_valid_score = float(get_validation_bleu(valid_preds))\n",
    "                best_valid_preds = copy.deepcopy(valid_preds)\n",
    "                best_test_preds = copy.deepcopy(get_test_predictions())\n",
    "                print 'Found new best validation score %f ' % (best_valid_score)\n",
    "            f.write('Epoch %d Minibatch %d BLEU on Validation : %s \\n' % (i, j, get_validation_bleu(valid_preds)))\n",
    "\n",
    "    if np.isnan(new_cost):\n",
    "        print 'NaN detected.'\n",
    "        break\n",
    "    valid_preds = get_validation_predictions()\n",
    "    print '==================================================================='\n",
    "    print 'Epoch %i BLEU on Validation : %s ' % (i, get_validation_bleu(valid_preds))\n",
    "    print '==================================================================='\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('output.txt', 'w')\n",
    "for line in best_test_preds:\n",
    "    f.write(' '.join(line) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'28.21'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_validation_bleu(valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu_stats(hypothesis, reference):\n",
    "    stats = []\n",
    "    stats.append(len(hypothesis))\n",
    "    stats.append(len(reference))\n",
    "    for n in xrange(1,5):\n",
    "        s_ngrams = Counter([tuple(hypothesis[i:i+n]) for i in xrange(len(hypothesis)+1-n)])\n",
    "        r_ngrams = Counter([tuple(reference[i:i+n]) for i in xrange(len(reference)+1-n)])\n",
    "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
    "        stats.append(max([len(hypothesis)+1-n, 0]))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu(stats):\n",
    "    if len(filter(lambda x: x==0, stats)) > 0:\n",
    "        return 0\n",
    "    (c, r) = stats[:2]\n",
    "    log_bleu_prec = sum([math.log(float(x)/y) for x,y in zip(stats[2::2],stats[3::2])]) / 4.\n",
    "    return math.exp(min([0, 1-float(r)/c]) + log_bleu_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_bleu(hypotheses):\n",
    "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    ref_lines = [line.strip().split() for line in open(path_to_dev_tgt, 'r')]\n",
    "    for hyp, ref in zip(hypotheses, ref_lines):\n",
    "        stats += np.array(bleu_stats(hyp, ref))\n",
    "    return \"%.2f\" % (100*bleu(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
