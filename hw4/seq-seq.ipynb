{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import sys\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/Users/sandeepsubramanian/CMU/UltraDeep/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from network import LSTM, FastLSTM\n",
    "from layer import HiddenLayer, EmbeddingLayer\n",
    "from learning_method import LearningMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_train_src = 'data/train.src'\n",
    "path_to_train_tgt = 'data/train.tgt'\n",
    "path_to_dev_src = 'data/dev.src'\n",
    "path_to_dev_tgt = 'data/dev.tgt'\n",
    "path_to_test_src = 'data/test.src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_src = [line.strip().split() for line in codecs.open(path_to_train_src, 'r', encoding='utf8')]\n",
    "train_tgt = [line.strip().split() for line in codecs.open(path_to_train_tgt, 'r', encoding='utf8')]\n",
    "dev_src = [line.strip().split() for line in codecs.open(path_to_dev_src, 'r', encoding='utf8')]\n",
    "dev_tgt = [line.strip().split() for line in codecs.open(path_to_dev_tgt, 'r', encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_vocab = set()\n",
    "for line in train_src:\n",
    "    for word in line:\n",
    "        src_vocab.add(word)\n",
    "\n",
    "src_word2ind = {}\n",
    "src_ind2word = {}\n",
    "\n",
    "for ind, word in enumerate(src_vocab):\n",
    "    src_word2ind[word] = ind\n",
    "    src_ind2word[ind] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_vocab = set()\n",
    "for line in train_tgt:\n",
    "    for word in line:\n",
    "        target_vocab.add(word)\n",
    "\n",
    "target_word2ind = {}\n",
    "target_ind2word = {}\n",
    "\n",
    "for ind, word in enumerate(target_vocab):\n",
    "    target_word2ind[word] = ind\n",
    "    target_ind2word[ind] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_inp = T.ivector()\n",
    "tgt_inp = T.ivector()\n",
    "tgt_op = T.ivector()\n",
    "#src_lengths = T.ivector()\n",
    "#tgt_mask = T.fmatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "src_embedding_layer = EmbeddingLayer(input_dim=len(src_word2ind), output_dim=64)\n",
    "tgt_embedding_layer = EmbeddingLayer(input_dim=len(target_word2ind), output_dim=64)\n",
    "src_lstm_forward = LSTM(input_dim=src_embedding_layer.output_dim, hidden_dim=128, with_batch=False)\n",
    "src_lstm_backward = LSTM(input_dim=src_embedding_layer.output_dim, hidden_dim=128, with_batch=False)\n",
    "tgt_lstm = LSTM(input_dim=tgt_embedding_layer.output_dim, hidden_dim=2 * src_lstm_forward.hidden_dim, with_batch=False)\n",
    "tgt_projection_layer = HiddenLayer(input_dim=tgt_lstm.hidden_dim * 2, output_dim=len(target_word2ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_inp_t = np.random.rand(5,).astype(np.int32)\n",
    "tgt_inp_t = np.random.rand(5,).astype(np.int32)\n",
    "tgt_op_t = np.random.rand(5,).astype(np.int32)\n",
    "#src_lengths_t = np.random.randint(0, 5, size=(10,)).astype(np.int32)\n",
    "#tgt_mask_t = np.float32(np.random.rand(10, 5).astype(np.float32) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 128 64 256\n"
     ]
    }
   ],
   "source": [
    "print src_lstm_forward.input_dim, src_lstm_forward.hidden_dim, tgt_lstm.input_dim, tgt_lstm.hidden_dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source embedding (5, 64)\n",
      "target embedding (5, 64)\n",
      "src lstm forward (5, 128)\n",
      "src lstm backward (5, 128)\n",
      "bilstm (5, 256)\n",
      "tgt lstm (5, 256)\n",
      "attention vectors (5, 256)\n",
      "proj rep (5, 3121)\n"
     ]
    }
   ],
   "source": [
    "# Get embedding matrices\n",
    "src_emb_inp = src_embedding_layer.link(src_inp)\n",
    "print 'source embedding', src_emb_inp.eval({src_inp:src_inp_t}).shape\n",
    "tgt_emb_inp = tgt_embedding_layer.link(tgt_inp)\n",
    "print 'target embedding', tgt_emb_inp.eval({tgt_inp:tgt_inp_t}).shape\n",
    "\n",
    "# Get BiLSTM representations\n",
    "src_lstm_forward.link(src_emb_inp)\n",
    "src_lstm_backward.link(src_emb_inp[::-1, :])\n",
    "encoder_representation = T.concatenate((src_lstm_forward.h, src_lstm_backward.h[::-1, :]), axis=1)\n",
    "print 'src lstm forward', src_lstm_forward.h.eval({src_inp:src_inp_t}).shape\n",
    "print 'src lstm backward', src_lstm_backward.h.eval({src_inp:src_inp_t}).shape\n",
    "print 'bilstm', encoder_representation.eval({src_inp:src_inp_t}).shape\n",
    "\n",
    "# Get Target LSTM representation & Attention Vectors\n",
    "tgt_lstm.h_0 = encoder_representation[-1]\n",
    "tgt_lstm.link(tgt_emb_inp)\n",
    "attention = tgt_lstm.h.dot(encoder_representation.transpose())\n",
    "attention = attention.dot(encoder_representation)\n",
    "print 'tgt lstm', tgt_lstm.h.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\n",
    "print 'attention vectors', attention.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\n",
    "\n",
    "# Concatenate the attention vectors to the Target LSTM output before predicting the next word\n",
    "target_representation = T.concatenate([attention, tgt_lstm.h], axis=1)\n",
    "\n",
    "# Predict each \n",
    "proj_output_rep = T.nnet.softmax(tgt_projection_layer.link(target_representation))\n",
    "print 'proj rep', proj_output_rep.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncost = - (T.log(proj_output_rep[\\n    T.arange(tgt_inp.shape[0]).dimshuffle(0, 'x').repeat(tgt_inp.shape[1], axis=1).flatten(),\\n    T.arange(tgt_inp.shape[1]).dimshuffle('x', 0).repeat(tgt_inp.shape[0], axis=0).flatten(),\\n    tgt_op.flatten()\\n]) * tgt_mask.flatten()).sum() / T.neq(tgt_mask, 0).sum()\\nprint cost.eval({tgt_inp:tgt_inp_t, tgt_mask:tgt_mask_t, tgt_op:tgt_op_t, src_inp:src_inp_t, src_lengths:src_lengths_t})\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cost = - (T.log(proj_output_rep[\n",
    "    T.arange(tgt_inp.shape[0]).dimshuffle(0, 'x').repeat(tgt_inp.shape[1], axis=1).flatten(),\n",
    "    T.arange(tgt_inp.shape[1]).dimshuffle('x', 0).repeat(tgt_inp.shape[0], axis=0).flatten(),\n",
    "    tgt_op.flatten()\n",
    "]) * tgt_mask.flatten()).sum() / T.neq(tgt_mask, 0).sum()\n",
    "print cost.eval({tgt_inp:tgt_inp_t, tgt_mask:tgt_mask_t, tgt_op:tgt_op_t, src_inp:src_inp_t, src_lengths:src_lengths_t})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.04578588964\n"
     ]
    }
   ],
   "source": [
    "cost = T.nnet.categorical_crossentropy(proj_output_rep, tgt_op).mean()\n",
    "print cost.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t, tgt_op:tgt_op_t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = src_embedding_layer.params + tgt_embedding_layer.params + src_lstm_forward.params + src_lstm_backward.params + tgt_lstm.params[:-1] + tgt_projection_layer.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates=LearningMethod(clip=5.0).get_updates('adam', cost, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_train = theano.function(\n",
    "    inputs=[src_inp, tgt_inp, tgt_op],\n",
    "    outputs=cost,\n",
    "    updates=updates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_eval = theano.function(\n",
    "    inputs=[src_inp, tgt_inp],\n",
    "    outputs=proj_output_rep,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(src_sents, tgt_sents, valid=False):\n",
    "    assert len(src_sents) == len(tgt_sents)\n",
    "    src_lengths = [len(sent) for sent in src_sents]\n",
    "    src_max_len = max(src_lengths)\n",
    "    if valid == False:\n",
    "        tgt_lengths = [len(sent) for sent in tgt_sents]\n",
    "        tgt_max_len = max(tgt_lengths)\n",
    "    return (\n",
    "        np.array([[src_word2ind[x] for x in sent] + ([0] * (src_max_len - len(sent))) for sent in src_sents]).astype(np.int32),\n",
    "        np.array(src_lengths).astype(np.int32),\n",
    "        np.array([[target_word2ind[x] for x in sent[:-1]] + ([0] * (tgt_max_len - len(sent))) for sent in tgt_sents]).astype(np.int32),\n",
    "        np.array([[target_word2ind[x] for x in sent[1:]] + ([0] * (tgt_max_len - len(sent))) for sent in tgt_sents]).astype(np.int32),\n",
    "        [([1] * (l - 1)) + ([0] * (tgt_max_len - l)) for l in tgt_lengths]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_validation_predictions():\n",
    "    validation_predictions = []    \n",
    "    for ind, sent in enumerate(dev_src):\n",
    "        \n",
    "        if ind % 300 == 0:\n",
    "            print ind, len(dev_src)\n",
    "        src_words = np.array([src_word2ind[x] for x in sent]).astype(np.int32)\n",
    "        current_outputs = [src_word2ind['<s>']]\n",
    "\n",
    "        while True:\n",
    "            next_word = f_eval(src_words, current_outputs).argmax(axis=1)[-1]\n",
    "            current_outputs.append(next_word)\n",
    "            if next_word == src_word2ind['</s>'] or len(current_outputs) >= 15:\n",
    "                validation_predictions.append([target_ind2word[x] for x in current_outputs])\n",
    "                break\n",
    "    return validation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "0 8.04610935803\n",
      "0 1006\n",
      "300 1006\n",
      "600"
     ]
    }
   ],
   "source": [
    "all_costs = []\n",
    "batch_size = 50\n",
    "n_epochs = 100\n",
    "for i in xrange(n_epochs):\n",
    "    print 'Starting epoch %i' % i\n",
    "    indices = range(len(train_src))\n",
    "    np.random.shuffle(indices)\n",
    "    train_src_batch = [train_src[ind] for ind in indices]\n",
    "    train_tgt_batch = [train_tgt[ind] for ind in indices]\n",
    "    assert len(train_src_batch) == len(train_tgt_batch)\n",
    "    costs = []\n",
    "    for j in xrange(len(train_src_batch)):\n",
    "        #s_sent, s_length, t_inp, t_op, mask = get_batch(train_src_batch[j:j + batch_size], train_tgt_batch[j:j+batch_size])\n",
    "        new_cost = f_train(\n",
    "            np.array([src_word2ind[x] for x in train_src_batch[j]]).astype(np.int32),\n",
    "            np.array([target_word2ind[x] for x in train_tgt_batch[j]][:-1]).astype(np.int32),\n",
    "            np.array([target_word2ind[x] for x in train_tgt_batch[j]][1:]).astype(np.int32),\n",
    "        )\n",
    "        all_costs.append((j, new_cost))\n",
    "        costs.append(new_cost)\n",
    "        if j % 300 == 0:\n",
    "            print j, np.mean(costs)\n",
    "            costs = []\n",
    "        if np.isnan(new_cost):\n",
    "            print 'NaN detected.'\n",
    "            break\n",
    "        if j % 10000 == 0:\n",
    "            valid_preds = get_validation_predictions()\n",
    "            print '==================================================================='\n",
    "            print 'Epoch %i BLEU on Validation : %s ' % (i, get_validation_bleu(valid_preds))\n",
    "            print '==================================================================='\n",
    "\n",
    "    if np.isnan(new_cost):\n",
    "        print 'NaN detected.'\n",
    "        break\n",
    "    valid_preds = get_validation_predictions()\n",
    "    print '==================================================================='\n",
    "    print 'Epoch %i BLEU on Validation : %s ' % (i, get_validation_bleu(valid_preds))\n",
    "    print '==================================================================='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"currency it 's a <unk> <unk> . </s> . </s> <unk> <unk> ? </s> .\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(valid_preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'<s> it is august fifteenth . </s>'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(dev_tgt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_batch_size = 100\n",
    "\n",
    "for i in xrange(0, len(dev_src), test_batch_size):\n",
    "    src_lengths = np.array([len(x) for x in dev_src[i:i+test_batch_size]]).astype(np.int32)\n",
    "    src_maxlen = np.max(src_lengths)\n",
    "    src_inps = np.array([ [src_word2ind[x] for x in dev_src[j]] + ([0] * (src_maxlen - len(dev_src[j]))) for j in xrange(i, i + test_batch_size)]).astype(np.int32)\n",
    "    current_outputs = [[target_word2ind['<s>']] for _ in xrange(len(features))]\n",
    "    final_outputs = [None] * len(features)\n",
    "\n",
    "    mapping = {j: j for j in xrange(len(features))}\n",
    "\n",
    "    while len(current_outputs) > 0:\n",
    "        to_delete = []\n",
    "        next_words = f_eval(src_inps, src_lengths, current_outputs)[:, -1, :].argmax(axis=1)\n",
    "        assert len(mapping) == len(next_words) == len(current_outputs)\n",
    "        for j in xrange(len(next_words)):\n",
    "            current_outputs[j].append(next_words[j])\n",
    "            if next_words[j] == target_word2ind['</s>'] or len(current_outputs[j]) >= 20:\n",
    "                final_outputs[mapping[j]] = current_outputs[j]\n",
    "                to_delete.append(j)\n",
    "        for j in sorted(to_delete)[::-1]:\n",
    "            del features[j]\n",
    "            del current_outputs[j]\n",
    "            del mapping[j]\n",
    "        new_index = 0\n",
    "        for k in sorted(mapping.keys()):\n",
    "            if k > new_index:\n",
    "                mapping[new_index] = mapping[k]\n",
    "                del mapping[k]\n",
    "            new_index += 1\n",
    "\n",
    "    assert all(final_outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx = f_eval(np.array([src_word2ind[x] for x in train_src_batch[0]]).astype(np.int32), np.array([target_word2ind[x] for x in train_tgt_batch[j]][:-1]).astype(np.int32)).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'can', u\"'s\", u'<unk>', u'<unk>', u'time', u'for', u'</s>']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[target_ind2word[_] for _ in xx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu_stats(hypothesis, reference):\n",
    "    stats = []\n",
    "    stats.append(len(hypothesis))\n",
    "    stats.append(len(reference))\n",
    "    for n in xrange(1,5):\n",
    "        s_ngrams = Counter([tuple(hypothesis[i:i+n]) for i in xrange(len(hypothesis)+1-n)])\n",
    "        r_ngrams = Counter([tuple(reference[i:i+n]) for i in xrange(len(reference)+1-n)])\n",
    "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
    "        stats.append(max([len(hypothesis)+1-n, 0]))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu(stats):\n",
    "    if len(filter(lambda x: x==0, stats)) > 0:\n",
    "        return 0\n",
    "    (c, r) = stats[:2]\n",
    "    log_bleu_prec = sum([math.log(float(x)/y) for x,y in zip(stats[2::2],stats[3::2])]) / 4.\n",
    "    return math.exp(min([0, 1-float(r)/c]) + log_bleu_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_bleu(hypotheses):\n",
    "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    ref_lines = [line.strip().split() for line in open(path_to_dev_tgt, 'r')]\n",
    "    for hyp, ref in zip(hypotheses, ref_lines):\n",
    "        stats += np.array(bleu_stats(hyp, ref))\n",
    "    return \"%.2f\" % (100*bleu(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
