{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import sys\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import math\n",
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use(\"gpu2\")\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/usr0/home/glample/Research/perso/UltraDeep/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from network import LSTM, FastLSTM\n",
    "from layer import HiddenLayer, EmbeddingLayer\n",
    "from learning_method import LearningMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_train_src = 'data/train.src'\n",
    "path_to_train_tgt = 'data/train.tgt'\n",
    "path_to_dev_src = 'data/dev.src'\n",
    "path_to_dev_tgt = 'data/dev.tgt'\n",
    "path_to_test_src = 'data/test.src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_src = [line.strip().split() for line in codecs.open(path_to_train_src, 'r', encoding='utf8')]\n",
    "train_tgt = [line.strip().split() for line in codecs.open(path_to_train_tgt, 'r', encoding='utf8')]\n",
    "dev_src = [line.strip().split() for line in codecs.open(path_to_dev_src, 'r', encoding='utf8')]\n",
    "dev_tgt = [line.strip().split() for line in codecs.open(path_to_dev_tgt, 'r', encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_vocab = set()\n",
    "for line in train_src:\n",
    "    for word in line:\n",
    "        src_vocab.add(word)\n",
    "\n",
    "src_word2ind = {}\n",
    "src_ind2word = {}\n",
    "\n",
    "for ind, word in enumerate(src_vocab):\n",
    "    src_word2ind[word] = ind\n",
    "    src_ind2word[ind] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_vocab = set()\n",
    "for line in train_tgt:\n",
    "    for word in line:\n",
    "        target_vocab.add(word)\n",
    "\n",
    "target_word2ind = {}\n",
    "target_ind2word = {}\n",
    "\n",
    "for ind, word in enumerate(target_vocab):\n",
    "    target_word2ind[word] = ind\n",
    "    target_ind2word[ind] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_src_temp = []\\nfor sentence in train_src:\\n    train_src_temp.append(np.array([src_word2ind[x] for x in sentence]).astype(np.int32))\\ntrain_src_temp = np.array(train_src_temp)\\ntrain_src = theano.shared(train_src_temp)\\n\\ntrain_tgt_temp = []\\nfor sentence in train_tgt:\\n    train_tgt_temp.append(np.array([target_word2ind[x] for x in sentence]).astype(np.int32))\\ntrain_tgt_temp = np.array(train_tgt_temp)\\ntrain_tgt = theano.shared(train_tgt_temp)\\n\\ndev_src_temp = []\\nfor sentence in dev_src:\\n    dev_src_temp.append(np.array([src_word2ind[x] for x in sentence]).astype(np.int32))\\ndev_src_temp = np.array(dev_src_temp)\\ndev_src = theano.shared(dev_src_temp)\\n\\ndev_tgt_temp = []\\nfor sentence in dev_tgt:\\n    dev_tgt_temp.append(np.array([target_word2ind[x] for x in sentence]).astype(np.int32))\\ndev_tgt_temp = np.array(dev_tgt_temp)\\ndev_tgt = theano.shared(dev_tgt_temp)\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_src_temp = []\n",
    "for sentence in train_src:\n",
    "    train_src_temp.append(np.array([src_word2ind[x] for x in sentence]).astype(np.int32))\n",
    "train_src_temp = np.array(train_src_temp)\n",
    "train_src = theano.shared(train_src_temp)\n",
    "\n",
    "train_tgt_temp = []\n",
    "for sentence in train_tgt:\n",
    "    train_tgt_temp.append(np.array([target_word2ind[x] for x in sentence]).astype(np.int32))\n",
    "train_tgt_temp = np.array(train_tgt_temp)\n",
    "train_tgt = theano.shared(train_tgt_temp)\n",
    "\n",
    "dev_src_temp = []\n",
    "for sentence in dev_src:\n",
    "    dev_src_temp.append(np.array([src_word2ind[x] for x in sentence]).astype(np.int32))\n",
    "dev_src_temp = np.array(dev_src_temp)\n",
    "dev_src = theano.shared(dev_src_temp)\n",
    "\n",
    "dev_tgt_temp = []\n",
    "for sentence in dev_tgt:\n",
    "    dev_tgt_temp.append(np.array([target_word2ind[x] for x in sentence]).astype(np.int32))\n",
    "dev_tgt_temp = np.array(dev_tgt_temp)\n",
    "dev_tgt = theano.shared(dev_tgt_temp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_inp = T.ivector()\n",
    "tgt_inp = T.ivector()\n",
    "tgt_op = T.ivector()\n",
    "index = T.scalar()\n",
    "#src_lengths = T.ivector()\n",
    "#tgt_mask = T.fmatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_embedding_layer = EmbeddingLayer(input_dim=len(src_word2ind), output_dim=64)\n",
    "tgt_embedding_layer = EmbeddingLayer(input_dim=len(target_word2ind), output_dim=64)\n",
    "src_lstm_forward = LSTM(input_dim=src_embedding_layer.output_dim, hidden_dim=128, with_batch=False)\n",
    "src_lstm_backward = LSTM(input_dim=src_embedding_layer.output_dim, hidden_dim=128, with_batch=False)\n",
    "tgt_lstm = LSTM(input_dim=tgt_embedding_layer.output_dim, hidden_dim=2 * src_lstm_forward.hidden_dim, with_batch=False)\n",
    "tgt_projection_layer = HiddenLayer(input_dim=tgt_lstm.hidden_dim * 2, output_dim=len(target_word2ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_inp_t = np.random.rand(5,).astype(np.int32)\n",
    "tgt_inp_t = np.random.rand(5,).astype(np.int32)\n",
    "tgt_op_t = np.random.rand(5,).astype(np.int32)\n",
    "#src_lengths_t = np.random.randint(0, 5, size=(10,)).astype(np.int32)\n",
    "#tgt_mask_t = np.float32(np.random.rand(10, 5).astype(np.float32) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 128 64 256\n"
     ]
    }
   ],
   "source": [
    "print src_lstm_forward.input_dim, src_lstm_forward.hidden_dim, tgt_lstm.input_dim, tgt_lstm.hidden_dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Get embedding matrices\\nsrc_emb_inp = src_embedding_layer.link(src_inp)\\nprint 'source embedding', src_emb_inp.eval({src_inp:src_inp_t}).shape\\ntgt_emb_inp = tgt_embedding_layer.link(tgt_inp)\\nprint 'target embedding', tgt_emb_inp.eval({tgt_inp:tgt_inp_t}).shape\\n\\n# Get BiLSTM representations\\nsrc_lstm_forward.link(src_emb_inp)\\nsrc_lstm_backward.link(src_emb_inp[::-1, :])\\nencoder_representation = T.concatenate((src_lstm_forward.h, src_lstm_backward.h[::-1, :]), axis=1)\\nprint 'src lstm forward', src_lstm_forward.h.eval({src_inp:src_inp_t}).shape\\nprint 'src lstm backward', src_lstm_backward.h.eval({src_inp:src_inp_t}).shape\\nprint 'bilstm', encoder_representation.eval({src_inp:src_inp_t}).shape\\n\\n# Get Target LSTM representation & Attention Vectors\\ntgt_lstm.h_0 = encoder_representation[-1]\\ntgt_lstm.link(tgt_emb_inp)\\nattention = tgt_lstm.h.dot(encoder_representation.transpose())\\nattention = attention.dot(encoder_representation)\\nprint 'tgt lstm', tgt_lstm.h.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\\nprint 'attention vectors', attention.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\\n\\n# Concatenate the attention vectors to the Target LSTM output before predicting the next word\\ntarget_representation = T.concatenate([attention, tgt_lstm.h], axis=1)\\n\\n# Predict each \\nproj_output_rep = T.nnet.softmax(tgt_projection_layer.link(target_representation))\\nprint 'proj rep', proj_output_rep.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\\n\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Get embedding matrices\n",
    "src_emb_inp = src_embedding_layer.link(src_inp)\n",
    "print 'source embedding', src_emb_inp.eval({src_inp:src_inp_t}).shape\n",
    "tgt_emb_inp = tgt_embedding_layer.link(tgt_inp)\n",
    "print 'target embedding', tgt_emb_inp.eval({tgt_inp:tgt_inp_t}).shape\n",
    "\n",
    "# Get BiLSTM representations\n",
    "src_lstm_forward.link(src_emb_inp)\n",
    "src_lstm_backward.link(src_emb_inp[::-1, :])\n",
    "encoder_representation = T.concatenate((src_lstm_forward.h, src_lstm_backward.h[::-1, :]), axis=1)\n",
    "print 'src lstm forward', src_lstm_forward.h.eval({src_inp:src_inp_t}).shape\n",
    "print 'src lstm backward', src_lstm_backward.h.eval({src_inp:src_inp_t}).shape\n",
    "print 'bilstm', encoder_representation.eval({src_inp:src_inp_t}).shape\n",
    "\n",
    "# Get Target LSTM representation & Attention Vectors\n",
    "tgt_lstm.h_0 = encoder_representation[-1]\n",
    "tgt_lstm.link(tgt_emb_inp)\n",
    "attention = tgt_lstm.h.dot(encoder_representation.transpose())\n",
    "attention = attention.dot(encoder_representation)\n",
    "print 'tgt lstm', tgt_lstm.h.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\n",
    "print 'attention vectors', attention.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\n",
    "\n",
    "# Concatenate the attention vectors to the Target LSTM output before predicting the next word\n",
    "target_representation = T.concatenate([attention, tgt_lstm.h], axis=1)\n",
    "\n",
    "# Predict each \n",
    "proj_output_rep = T.nnet.softmax(tgt_projection_layer.link(target_representation))\n",
    "print 'proj rep', proj_output_rep.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Model\n",
    "#\n",
    "src_emb_dim      = 256  # source word embedding dimension\n",
    "tgt_emb_dim      = 256  # target word embedding dimension\n",
    "src_lstm_hid_dim = 512  # source LSTMs hidden dimension\n",
    "tgt_lstm_hid_dim = 2 * src_lstm_hid_dim  # target LSTM hidden dimension\n",
    "proj_dim         = 104  # size of the first projection layer\n",
    "dropout          = 0.5  # dropout rate\n",
    "\n",
    "n_src = len(src_word2ind)  # number of words in the source language\n",
    "n_tgt = len(target_word2ind)  # number of words in the target language\n",
    "\n",
    "# Parameters\n",
    "params = []\n",
    "\n",
    "# Source words + target words embeddings layer\n",
    "src_lookup = EmbeddingLayer(n_src, src_emb_dim, name='src_lookup') # lookup table for source words\n",
    "tgt_lookup = EmbeddingLayer(n_tgt, tgt_emb_dim, name='tgt_lookup') # lookup table for target words\n",
    "params += src_lookup.params + tgt_lookup.params\n",
    "\n",
    "# LSTMs\n",
    "src_lstm_for = LSTM(src_emb_dim, src_lstm_hid_dim, name='src_lstm_for', with_batch=False)\n",
    "src_lstm_rev = LSTM(src_emb_dim, src_lstm_hid_dim, name='src_lstm_rev', with_batch=False)\n",
    "tgt_lstm = LSTM(2 * tgt_emb_dim, tgt_lstm_hid_dim, name='tgt_lstm', with_batch=False)\n",
    "params += src_lstm_for.params + src_lstm_rev.params + tgt_lstm.params[:-1]\n",
    "\n",
    "# Projection layers\n",
    "proj_layer1 = HiddenLayer(tgt_lstm_hid_dim + 2 * src_lstm_hid_dim, n_tgt, name='proj_layer1', activation='softmax')\n",
    "proj_layer2 = HiddenLayer(2 * src_lstm_hid_dim, tgt_emb_dim, name='proj_layer2', activation='tanh')\n",
    "params += proj_layer1.params # + proj_layer2.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_sentence_emb (3, 256)\n",
      "tgt_sentence_emb (5, 256)\n",
      "src_lstm_for.h (3, 512)\n",
      "src_lstm_rev.h (3, 512)\n",
      "src_context (3, 1024)\n",
      "repeated src_context (5, 256)\n",
      "tgt sentence emb (5, 512)\n",
      "tgt_lstm.h (5, 1024)\n",
      "transition (5, 1024)\n",
      "transition_last (5, 2048)\n",
      "prediction (5, 3121)\n",
      "cost 8.04848766327\n"
     ]
    }
   ],
   "source": [
    "is_train_t = 1\n",
    "\n",
    "src_sentence_t = [3, 4, 2]\n",
    "tgt_sentence_t = [1, 8, 0, 8, 2]\n",
    "tgt_gold_t = [1, 3, 2, 2, 1]\n",
    "\n",
    "\n",
    "# Train status\n",
    "is_train = T.iscalar('is_train')\n",
    "# Input sentence\n",
    "src_sentence = T.ivector()\n",
    "# Current output translation\n",
    "tgt_sentence = T.ivector()\n",
    "# Gold translation\n",
    "tgt_gold = T.ivector()\n",
    "\n",
    "src_sentence_emb = src_lookup.link(src_sentence)\n",
    "tgt_sentence_emb = tgt_lookup.link(tgt_sentence)\n",
    "print 'src_sentence_emb', src_sentence_emb.eval({src_sentence: src_sentence_t}).shape\n",
    "print 'tgt_sentence_emb', tgt_sentence_emb.eval({tgt_sentence: tgt_sentence_t}).shape\n",
    "\n",
    "src_lstm_for.link(src_sentence_emb)\n",
    "src_lstm_rev.link(src_sentence_emb[::-1, :])\n",
    "\n",
    "print 'src_lstm_for.h', src_lstm_for.h.eval({src_sentence: src_sentence_t}).shape\n",
    "print 'src_lstm_rev.h', src_lstm_rev.h.eval({src_sentence: src_sentence_t}).shape\n",
    "\n",
    "src_context = T.concatenate([src_lstm_for.h, src_lstm_rev.h[::-1, :]], axis=1)\n",
    "print 'src_context', src_context.eval({src_sentence: src_sentence_t}).shape\n",
    "\n",
    "tgt_lstm.h_0 = src_context[-1]\n",
    "repeated_src_context = T.repeat(src_context[-1].dimshuffle('x', 0), tgt_sentence_emb.shape[0], axis=0)\n",
    "repeated_src_context = proj_layer2.link(repeated_src_context)\n",
    "print 'repeated src_context', repeated_src_context.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "tgt_sentence_emb = T.concatenate((tgt_sentence_emb, repeated_src_context), axis=1)\n",
    "print 'tgt sentence emb', tgt_sentence_emb.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "tgt_lstm.link(tgt_sentence_emb)\n",
    "print 'tgt_lstm.h', tgt_lstm.h.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "\n",
    "transition = tgt_lstm.h.dot(src_context.transpose())\n",
    "transition = transition.dot(src_context)\n",
    "print 'transition', transition.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "# print 'transition_matrix', transition_matrix.eval({src_sentence: src_sentence_t}).shape\n",
    "# print 'transition_matrix.dot(tgt_lstm.output)', src_context.transpose().dot(src_context.dot(tgt_lstm.output)).eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "# print 'transition_matrix.dot(tgt_lstm.output)', tgt_lstm.h.dot(transition_matrix).eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "\n",
    "transition_last = T.concatenate([transition, tgt_lstm.h], axis=1)\n",
    "print 'transition_last', transition_last.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "\n",
    "prediction = proj_layer1.link(transition_last)\n",
    "print 'prediction', prediction.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "\n",
    "cost = T.nnet.categorical_crossentropy(prediction, tgt_gold).mean()\n",
    "print 'cost', cost.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t, tgt_gold: tgt_gold_t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncost = - (T.log(proj_output_rep[\\n    T.arange(tgt_inp.shape[0]).dimshuffle(0, 'x').repeat(tgt_inp.shape[1], axis=1).flatten(),\\n    T.arange(tgt_inp.shape[1]).dimshuffle('x', 0).repeat(tgt_inp.shape[0], axis=0).flatten(),\\n    tgt_op.flatten()\\n]) * tgt_mask.flatten()).sum() / T.neq(tgt_mask, 0).sum()\\nprint cost.eval({tgt_inp:tgt_inp_t, tgt_mask:tgt_mask_t, tgt_op:tgt_op_t, src_inp:src_inp_t, src_lengths:src_lengths_t})\\n\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cost = - (T.log(proj_output_rep[\n",
    "    T.arange(tgt_inp.shape[0]).dimshuffle(0, 'x').repeat(tgt_inp.shape[1], axis=1).flatten(),\n",
    "    T.arange(tgt_inp.shape[1]).dimshuffle('x', 0).repeat(tgt_inp.shape[0], axis=0).flatten(),\n",
    "    tgt_op.flatten()\n",
    "]) * tgt_mask.flatten()).sum() / T.neq(tgt_mask, 0).sum()\n",
    "print cost.eval({tgt_inp:tgt_inp_t, tgt_mask:tgt_mask_t, tgt_op:tgt_op_t, src_inp:src_inp_t, src_lengths:src_lengths_t})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cost = T.nnet.categorical_crossentropy(proj_output_rep, tgt_op).mean()\n",
    "#print cost.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t, tgt_op:tgt_op_t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#params = src_embedding_layer.params + tgt_embedding_layer.params + src_lstm_forward.params + src_lstm_backward.params + tgt_lstm.params[:-1] + tgt_projection_layer.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates=LearningMethod(clip=5.0).get_updates('adam', cost, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_train = theano.function(\n",
    "    inputs=[src_sentence, tgt_sentence, tgt_gold],\n",
    "    outputs=cost,\n",
    "    updates=updates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_eval = theano.function(\n",
    "    inputs=[src_sentence, tgt_sentence],\n",
    "    outputs=prediction,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(src_sents, tgt_sents, valid=False):\n",
    "    assert len(src_sents) == len(tgt_sents)\n",
    "    src_lengths = [len(sent) for sent in src_sents]\n",
    "    src_max_len = max(src_lengths)\n",
    "    if valid == False:\n",
    "        tgt_lengths = [len(sent) for sent in tgt_sents]\n",
    "        tgt_max_len = max(tgt_lengths)\n",
    "    return (\n",
    "        np.array([[src_word2ind[x] for x in sent] + ([0] * (src_max_len - len(sent))) for sent in src_sents]).astype(np.int32),\n",
    "        np.array(src_lengths).astype(np.int32),\n",
    "        np.array([[target_word2ind[x] for x in sent[:-1]] + ([0] * (tgt_max_len - len(sent))) for sent in tgt_sents]).astype(np.int32),\n",
    "        np.array([[target_word2ind[x] for x in sent[1:]] + ([0] * (tgt_max_len - len(sent))) for sent in tgt_sents]).astype(np.int32),\n",
    "        [([1] * (l - 1)) + ([0] * (tgt_max_len - l)) for l in tgt_lengths]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_validation_predictions():\n",
    "    validation_predictions = []    \n",
    "    for ind, sent in enumerate(dev_src):\n",
    "        \n",
    "        if ind % 300 == 0:\n",
    "            print ind, len(dev_src)\n",
    "        src_words = np.array([src_word2ind[x] for x in sent]).astype(np.int32)\n",
    "        current_outputs = [target_word2ind['<s>']]\n",
    "\n",
    "        while True:\n",
    "            next_word = f_eval(src_words, current_outputs).argmax(axis=1)[-1]\n",
    "            current_outputs.append(next_word)\n",
    "            #print [target_ind2word[x] for x in current_outputs]\n",
    "            if next_word == target_word2ind['</s>'] or len(current_outputs) >= 15:\n",
    "                validation_predictions.append([target_ind2word[x] for x in current_outputs])\n",
    "                break\n",
    "    return validation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_predictions():\n",
    "    test_predictions = []    \n",
    "    for ind, sent in enumerate(test_):\n",
    "        \n",
    "        if ind % 300 == 0:\n",
    "            print ind, len(dev_src)\n",
    "        src_words = np.array([src_word2ind[x] for x in sent]).astype(np.int32)\n",
    "        current_outputs = [target_word2ind['<s>']]\n",
    "\n",
    "        while True:\n",
    "            next_word = f_eval(src_words, current_outputs).argmax(axis=1)[-1]\n",
    "            current_outputs.append(next_word)\n",
    "            #print [target_ind2word[x] for x in current_outputs]\n",
    "            if next_word == target_word2ind['</s>'] or len(current_outputs) >= 15:\n",
    "                validation_predictions.append([target_ind2word[x] for x in current_outputs])\n",
    "                break\n",
    "    return validation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "0 0.633995\n",
      "0 1006\n",
      "300 1006\n",
      "600 1006\n",
      "900"
     ]
    }
   ],
   "source": [
    "f = open('blue_valid_log.txt', 'w')\n",
    "all_costs = []\n",
    "batch_size = 50\n",
    "n_epochs = 100\n",
    "best_valid_preds = None\n",
    "best_valid_score = -sys.maxint\n",
    "for i in xrange(n_epochs):\n",
    "    print 'Starting epoch %i' % i\n",
    "    indices = range(len(train_src))\n",
    "    np.random.shuffle(indices)\n",
    "    train_src_batch = [train_src[ind] for ind in indices]\n",
    "    train_tgt_batch = [train_tgt[ind] for ind in indices]\n",
    "    assert len(train_src_batch) == len(train_tgt_batch)\n",
    "    costs = []\n",
    "    for j in xrange(len(train_src_batch)):\n",
    "        #s_sent, s_length, t_inp, t_op, mask = get_batch(train_src_batch[j:j + batch_size], train_tgt_batch[j:j+batch_size])\n",
    "        new_cost = f_train(\n",
    "            np.array([src_word2ind[x] for x in train_src_batch[j]]).astype(np.int32),\n",
    "            np.array([target_word2ind[x] for x in train_tgt_batch[j]][:-1]).astype(np.int32),\n",
    "            np.array([target_word2ind[x] for x in train_tgt_batch[j]][1:]).astype(np.int32),\n",
    "        )\n",
    "        all_costs.append((j, new_cost))\n",
    "        costs.append(new_cost)\n",
    "        if j % 300 == 0:\n",
    "            print j, np.mean(costs)\n",
    "            costs = []\n",
    "        if np.isnan(new_cost):\n",
    "            print 'NaN detected.'\n",
    "            break\n",
    "        if j % 10000 == 0:\n",
    "            valid_preds = get_validation_predictions()\n",
    "            print '==================================================================='\n",
    "            print 'Epoch %i BLEU on Validation : %s ' % (i, get_validation_bleu(valid_preds))\n",
    "            print '==================================================================='\n",
    "            if float(get_validation_bleu(valid_preds)) >= best_valid_score:\n",
    "                best_valid_score = float(get_validation_bleu(valid_preds))\n",
    "                best_valid_preds = copy.deepcopy(valid_preds)\n",
    "                print 'Found new best validation score %f ' % (best_valid_score)\n",
    "            f.write('Epoch %d Minibatch %d BLEU on Validation : %s \\n' % (i, j, get_validation_bleu(valid_preds)))\n",
    "\n",
    "    if np.isnan(new_cost):\n",
    "        print 'NaN detected.'\n",
    "        break\n",
    "    valid_preds = get_validation_predictions()\n",
    "    print '==================================================================='\n",
    "    print 'Epoch %i BLEU on Validation : %s ' % (i, get_validation_bleu(valid_preds))\n",
    "    print '==================================================================='\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.91"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_batch_size = 100\n",
    "\n",
    "for i in xrange(0, len(dev_src), test_batch_size):\n",
    "    src_lengths = np.array([len(x) for x in dev_src[i:i+test_batch_size]]).astype(np.int32)\n",
    "    src_maxlen = np.max(src_lengths)\n",
    "    src_inps = np.array([ [src_word2ind[x] for x in dev_src[j]] + ([0] * (src_maxlen - len(dev_src[j]))) for j in xrange(i, i + test_batch_size)]).astype(np.int32)\n",
    "    current_outputs = [[target_word2ind['<s>']] for _ in xrange(len(features))]\n",
    "    final_outputs = [None] * len(features)\n",
    "\n",
    "    mapping = {j: j for j in xrange(len(features))}\n",
    "\n",
    "    while len(current_outputs) > 0:\n",
    "        to_delete = []\n",
    "        next_words = f_eval(src_inps, src_lengths, current_outputs)[:, -1, :].argmax(axis=1)\n",
    "        assert len(mapping) == len(next_words) == len(current_outputs)\n",
    "        for j in xrange(len(next_words)):\n",
    "            current_outputs[j].append(next_words[j])\n",
    "            if next_words[j] == target_word2ind['</s>'] or len(current_outputs[j]) >= 20:\n",
    "                final_outputs[mapping[j]] = current_outputs[j]\n",
    "                to_delete.append(j)\n",
    "        for j in sorted(to_delete)[::-1]:\n",
    "            del features[j]\n",
    "            del current_outputs[j]\n",
    "            del mapping[j]\n",
    "        new_index = 0\n",
    "        for k in sorted(mapping.keys()):\n",
    "            if k > new_index:\n",
    "                mapping[new_index] = mapping[k]\n",
    "                del mapping[k]\n",
    "            new_index += 1\n",
    "\n",
    "    assert all(final_outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx = f_eval(np.array([src_word2ind[x] for x in dev_src[5]]).astype(np.int32), np.array([target_word2ind[x] for x in dev_tgt[5]][:-1]).astype(np.int32)).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'how',\n",
       " u'do',\n",
       " u'i',\n",
       " u'use',\n",
       " u'the',\n",
       " u'safe-',\n",
       " u'restaurant',\n",
       " u'box',\n",
       " u'?',\n",
       " u'</s>']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[target_ind2word[_] for _ in xx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'<s>',\n",
       " u'how',\n",
       " u'do',\n",
       " u'i',\n",
       " u'use',\n",
       " u'a',\n",
       " u'safe',\n",
       " u'deposit',\n",
       " u'box',\n",
       " u'?',\n",
       " u'</s>']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_tgt[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1006\n",
      "300 1006\n",
      "600 1006\n",
      "900 1006\n"
     ]
    }
   ],
   "source": [
    "valid_preds = get_validation_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'28.21'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_validation_bleu(valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currency my throbbing shower is <unk> . </s> nine four five . </s> all the\n",
      "currency a lot of august fifteenth . </s> number five . </s> one of them\n",
      "currency for me to japan ? </s> over the stairs to japan . </s> .\n",
      "currency one hundred dollars in a night . </s> . </s> </s> </s> </s> <unk>\n",
      "currency on a newspaper ? </s> up with a newspaper . </s> </s> <unk> .\n",
      "currency for <unk> ? </s> </s> three ? </s> </s> <unk> ? </s> </s> <unk>\n",
      "currency where where the boarding is where ? </s> the thirty- four ? </s> </s>\n",
      "currency a table is free to go . </s> if it 's available ? </s>\n",
      "currency these are <unk> over . </s> </s> will be <unk> . </s> </s> <unk>\n",
      "currency be an hour . </s> 's <unk> . </s> ? </s> </s> <unk> .\n"
     ]
    }
   ],
   "source": [
    "for x in valid_preds[:10]:\n",
    "    print ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu_stats(hypothesis, reference):\n",
    "    stats = []\n",
    "    stats.append(len(hypothesis))\n",
    "    stats.append(len(reference))\n",
    "    for n in xrange(1,5):\n",
    "        s_ngrams = Counter([tuple(hypothesis[i:i+n]) for i in xrange(len(hypothesis)+1-n)])\n",
    "        r_ngrams = Counter([tuple(reference[i:i+n]) for i in xrange(len(reference)+1-n)])\n",
    "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
    "        stats.append(max([len(hypothesis)+1-n, 0]))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu(stats):\n",
    "    if len(filter(lambda x: x==0, stats)) > 0:\n",
    "        return 0\n",
    "    (c, r) = stats[:2]\n",
    "    log_bleu_prec = sum([math.log(float(x)/y) for x,y in zip(stats[2::2],stats[3::2])]) / 4.\n",
    "    return math.exp(min([0, 1-float(r)/c]) + log_bleu_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_bleu(hypotheses):\n",
    "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    ref_lines = [line.strip().split() for line in open(path_to_dev_tgt, 'r')]\n",
    "    for hyp, ref in zip(hypotheses, ref_lines):\n",
    "        stats += np.array(bleu_stats(hyp, ref))\n",
    "    return \"%.2f\" % (100*bleu(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
