{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '90186' (I am process '30291')\n",
      "WARNING:theano.gof.compilelock:Overriding existing lock by dead process '90186' (I am process '30291')\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import sys\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import math\n",
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use(\"gpu0\")\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/usr1/home/ssandeep/UltraDeep/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from network import LSTM, FastLSTM\n",
    "from layer import HiddenLayer, EmbeddingLayer\n",
    "from learning_method import LearningMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_train_src = 'data/train.src'\n",
    "path_to_train_tgt = 'data/train.tgt'\n",
    "path_to_dev_src = 'data/dev.src'\n",
    "path_to_dev_tgt = 'data/dev.tgt'\n",
    "path_to_test_src = 'data/test.src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_src = [line.strip().split() for line in codecs.open(path_to_train_src, 'r', encoding='utf8')]\n",
    "train_tgt = [line.strip().split() for line in codecs.open(path_to_train_tgt, 'r', encoding='utf8')]\n",
    "dev_src = [line.strip().split() for line in codecs.open(path_to_dev_src, 'r', encoding='utf8')]\n",
    "dev_tgt = [line.strip().split() for line in codecs.open(path_to_dev_tgt, 'r', encoding='utf8')]\n",
    "test_src = [line.strip().split() for line in codecs.open(path_to_test_src, 'r', encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_vocab = set()\n",
    "for line in train_src:\n",
    "    for word in line:\n",
    "        src_vocab.add(word)\n",
    "\n",
    "src_word2ind = {}\n",
    "src_ind2word = {}\n",
    "\n",
    "for ind, word in enumerate(src_vocab):\n",
    "    src_word2ind[word] = ind\n",
    "    src_ind2word[ind] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_vocab = set()\n",
    "for line in train_tgt:\n",
    "    for word in line:\n",
    "        target_vocab.add(word)\n",
    "\n",
    "target_word2ind = {}\n",
    "target_ind2word = {}\n",
    "\n",
    "for ind, word in enumerate(target_vocab):\n",
    "    target_word2ind[word] = ind\n",
    "    target_ind2word[ind] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_inp = T.ivector()\n",
    "tgt_inp = T.ivector()\n",
    "tgt_op = T.ivector()\n",
    "index = T.scalar()\n",
    "#src_lengths = T.ivector()\n",
    "#tgt_mask = T.fmatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_embedding_layer = EmbeddingLayer(input_dim=len(src_word2ind), output_dim=64)\n",
    "tgt_embedding_layer = EmbeddingLayer(input_dim=len(target_word2ind), output_dim=64)\n",
    "src_lstm_forward = LSTM(input_dim=src_embedding_layer.output_dim, hidden_dim=128, with_batch=False)\n",
    "src_lstm_backward = LSTM(input_dim=src_embedding_layer.output_dim, hidden_dim=128, with_batch=False)\n",
    "tgt_lstm = LSTM(input_dim=tgt_embedding_layer.output_dim, hidden_dim=2 * src_lstm_forward.hidden_dim, with_batch=False)\n",
    "tgt_projection_layer = HiddenLayer(input_dim=tgt_lstm.hidden_dim * 2, output_dim=len(target_word2ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_inp_t = np.random.rand(5,).astype(np.int32)\n",
    "tgt_inp_t = np.random.rand(5,).astype(np.int32)\n",
    "tgt_op_t = np.random.rand(5,).astype(np.int32)\n",
    "#src_lengths_t = np.random.randint(0, 5, size=(10,)).astype(np.int32)\n",
    "#tgt_mask_t = np.float32(np.random.rand(10, 5).astype(np.float32) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 128 64 256\n"
     ]
    }
   ],
   "source": [
    "print src_lstm_forward.input_dim, src_lstm_forward.hidden_dim, tgt_lstm.input_dim, tgt_lstm.hidden_dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Get embedding matrices\\nsrc_emb_inp = src_embedding_layer.link(src_inp)\\nprint 'source embedding', src_emb_inp.eval({src_inp:src_inp_t}).shape\\ntgt_emb_inp = tgt_embedding_layer.link(tgt_inp)\\nprint 'target embedding', tgt_emb_inp.eval({tgt_inp:tgt_inp_t}).shape\\n\\n# Get BiLSTM representations\\nsrc_lstm_forward.link(src_emb_inp)\\nsrc_lstm_backward.link(src_emb_inp[::-1, :])\\nencoder_representation = T.concatenate((src_lstm_forward.h, src_lstm_backward.h[::-1, :]), axis=1)\\nprint 'src lstm forward', src_lstm_forward.h.eval({src_inp:src_inp_t}).shape\\nprint 'src lstm backward', src_lstm_backward.h.eval({src_inp:src_inp_t}).shape\\nprint 'bilstm', encoder_representation.eval({src_inp:src_inp_t}).shape\\n\\n# Get Target LSTM representation & Attention Vectors\\ntgt_lstm.h_0 = encoder_representation[-1]\\ntgt_lstm.link(tgt_emb_inp)\\nattention = tgt_lstm.h.dot(encoder_representation.transpose())\\nattention = attention.dot(encoder_representation)\\nprint 'tgt lstm', tgt_lstm.h.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\\nprint 'attention vectors', attention.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\\n\\n# Concatenate the attention vectors to the Target LSTM output before predicting the next word\\ntarget_representation = T.concatenate([attention, tgt_lstm.h], axis=1)\\n\\n# Predict each \\nproj_output_rep = T.nnet.softmax(tgt_projection_layer.link(target_representation))\\nprint 'proj rep', proj_output_rep.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Get embedding matrices\n",
    "src_emb_inp = src_embedding_layer.link(src_inp)\n",
    "print 'source embedding', src_emb_inp.eval({src_inp:src_inp_t}).shape\n",
    "tgt_emb_inp = tgt_embedding_layer.link(tgt_inp)\n",
    "print 'target embedding', tgt_emb_inp.eval({tgt_inp:tgt_inp_t}).shape\n",
    "\n",
    "# Get BiLSTM representations\n",
    "src_lstm_forward.link(src_emb_inp)\n",
    "src_lstm_backward.link(src_emb_inp[::-1, :])\n",
    "encoder_representation = T.concatenate((src_lstm_forward.h, src_lstm_backward.h[::-1, :]), axis=1)\n",
    "print 'src lstm forward', src_lstm_forward.h.eval({src_inp:src_inp_t}).shape\n",
    "print 'src lstm backward', src_lstm_backward.h.eval({src_inp:src_inp_t}).shape\n",
    "print 'bilstm', encoder_representation.eval({src_inp:src_inp_t}).shape\n",
    "\n",
    "# Get Target LSTM representation & Attention Vectors\n",
    "tgt_lstm.h_0 = encoder_representation[-1]\n",
    "tgt_lstm.link(tgt_emb_inp)\n",
    "attention = tgt_lstm.h.dot(encoder_representation.transpose())\n",
    "attention = attention.dot(encoder_representation)\n",
    "print 'tgt lstm', tgt_lstm.h.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\n",
    "print 'attention vectors', attention.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\n",
    "\n",
    "# Concatenate the attention vectors to the Target LSTM output before predicting the next word\n",
    "target_representation = T.concatenate([attention, tgt_lstm.h], axis=1)\n",
    "\n",
    "# Predict each \n",
    "proj_output_rep = T.nnet.softmax(tgt_projection_layer.link(target_representation))\n",
    "print 'proj rep', proj_output_rep.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t}).shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Model\n",
    "#\n",
    "src_emb_dim      = 256  # source word embedding dimension\n",
    "tgt_emb_dim      = 256  # target word embedding dimension\n",
    "src_lstm_hid_dim = 512  # source LSTMs hidden dimension\n",
    "tgt_lstm_hid_dim = 2 * src_lstm_hid_dim  # target LSTM hidden dimension\n",
    "proj_dim         = 104  # size of the first projection layer\n",
    "dropout          = 0.5  # dropout rate\n",
    "\n",
    "n_src = len(src_word2ind)  # number of words in the source language\n",
    "n_tgt = len(target_word2ind)  # number of words in the target language\n",
    "\n",
    "# Parameters\n",
    "params = []\n",
    "\n",
    "# Source words + target words embeddings layer\n",
    "src_lookup = EmbeddingLayer(n_src, src_emb_dim, name='src_lookup') # lookup table for source words\n",
    "tgt_lookup = EmbeddingLayer(n_tgt, tgt_emb_dim, name='tgt_lookup') # lookup table for target words\n",
    "params += src_lookup.params + tgt_lookup.params\n",
    "\n",
    "# LSTMs\n",
    "src_lstm_for = LSTM(src_emb_dim, src_lstm_hid_dim, name='src_lstm_for', with_batch=False)\n",
    "src_lstm_rev = LSTM(src_emb_dim, src_lstm_hid_dim, name='src_lstm_rev', with_batch=False)\n",
    "tgt_lstm = LSTM(2 * tgt_emb_dim, tgt_lstm_hid_dim, name='tgt_lstm', with_batch=False)\n",
    "params += src_lstm_for.params + src_lstm_rev.params + tgt_lstm.params[:-1]\n",
    "\n",
    "# Projection layers\n",
    "proj_layer1 = HiddenLayer(tgt_lstm_hid_dim + 2 * src_lstm_hid_dim, n_tgt, name='proj_layer1', activation='softmax')\n",
    "proj_layer2 = HiddenLayer(2 * src_lstm_hid_dim, tgt_emb_dim, name='proj_layer2', activation='tanh')\n",
    "params += proj_layer1.params # + proj_layer2.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_sentence_emb (3, 256)\n",
      "tgt_sentence_emb (5, 256)\n",
      "src_lstm_for.h (3, 512)\n",
      "src_lstm_rev.h (3, 512)\n",
      "src_context (3, 1024)\n",
      "repeated src_context (5, 256)\n",
      "tgt sentence emb (5, 512)\n",
      "tgt_lstm.h (5, 1024)\n",
      "transition (5, 1024)\n",
      "transition_last (5, 2048)\n",
      "prediction (5, 3121)\n",
      "cost 8.04461860657\n"
     ]
    }
   ],
   "source": [
    "is_train_t = 1\n",
    "\n",
    "src_sentence_t = [3, 4, 2]\n",
    "tgt_sentence_t = [1, 8, 0, 8, 2]\n",
    "tgt_gold_t = [1, 3, 2, 2, 1]\n",
    "\n",
    "beta = 500\n",
    "\n",
    "# Train status\n",
    "is_train = T.iscalar('is_train')\n",
    "# Input sentence\n",
    "src_sentence = T.ivector()\n",
    "# Current output translation\n",
    "tgt_sentence = T.ivector()\n",
    "# Gold translation\n",
    "tgt_gold = T.ivector()\n",
    "\n",
    "src_sentence_emb = src_lookup.link(src_sentence)\n",
    "tgt_sentence_emb = tgt_lookup.link(tgt_sentence)\n",
    "print 'src_sentence_emb', src_sentence_emb.eval({src_sentence: src_sentence_t}).shape\n",
    "print 'tgt_sentence_emb', tgt_sentence_emb.eval({tgt_sentence: tgt_sentence_t}).shape\n",
    "\n",
    "src_lstm_for.link(src_sentence_emb)\n",
    "src_lstm_rev.link(src_sentence_emb[::-1, :])\n",
    "\n",
    "print 'src_lstm_for.h', src_lstm_for.h.eval({src_sentence: src_sentence_t}).shape\n",
    "print 'src_lstm_rev.h', src_lstm_rev.h.eval({src_sentence: src_sentence_t}).shape\n",
    "\n",
    "src_context = T.concatenate([src_lstm_for.h, src_lstm_rev.h[::-1, :]], axis=1)\n",
    "print 'src_context', src_context.eval({src_sentence: src_sentence_t}).shape\n",
    "\n",
    "tgt_lstm.h_0 = src_context[-1]\n",
    "repeated_src_context = T.repeat(src_context[-1].dimshuffle('x', 0), tgt_sentence_emb.shape[0], axis=0)\n",
    "repeated_src_context = proj_layer2.link(repeated_src_context)\n",
    "print 'repeated src_context', repeated_src_context.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "tgt_sentence_emb = T.concatenate((tgt_sentence_emb, repeated_src_context), axis=1)\n",
    "print 'tgt sentence emb', tgt_sentence_emb.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "tgt_lstm.link(tgt_sentence_emb)\n",
    "print 'tgt_lstm.h', tgt_lstm.h.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "\n",
    "transition = tgt_lstm.h.dot(src_context.transpose())\n",
    "transition = transition.dot(src_context)\n",
    "print 'transition', transition.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "# print 'transition_matrix', transition_matrix.eval({src_sentence: src_sentence_t}).shape\n",
    "# print 'transition_matrix.dot(tgt_lstm.output)', src_context.transpose().dot(src_context.dot(tgt_lstm.output)).eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "# print 'transition_matrix.dot(tgt_lstm.output)', tgt_lstm.h.dot(transition_matrix).eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "\n",
    "transition_last = T.concatenate([transition, tgt_lstm.h], axis=1)\n",
    "print 'transition_last', transition_last.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "\n",
    "prediction = proj_layer1.link(transition_last)\n",
    "print 'prediction', prediction.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t}).shape\n",
    "\n",
    "cost = T.nnet.categorical_crossentropy(prediction, tgt_gold).mean()\n",
    "cost += beta * T.mean((tgt_lstm.h[:-1] ** 2 - tgt_lstm.h[1:] ** 2) ** 2)\n",
    "print 'cost', cost.eval({src_sentence: src_sentence_t, tgt_sentence: tgt_sentence_t, tgt_gold: tgt_gold_t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncost = - (T.log(proj_output_rep[\\n    T.arange(tgt_inp.shape[0]).dimshuffle(0, 'x').repeat(tgt_inp.shape[1], axis=1).flatten(),\\n    T.arange(tgt_inp.shape[1]).dimshuffle('x', 0).repeat(tgt_inp.shape[0], axis=0).flatten(),\\n    tgt_op.flatten()\\n]) * tgt_mask.flatten()).sum() / T.neq(tgt_mask, 0).sum()\\nprint cost.eval({tgt_inp:tgt_inp_t, tgt_mask:tgt_mask_t, tgt_op:tgt_op_t, src_inp:src_inp_t, src_lengths:src_lengths_t})\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cost = - (T.log(proj_output_rep[\n",
    "    T.arange(tgt_inp.shape[0]).dimshuffle(0, 'x').repeat(tgt_inp.shape[1], axis=1).flatten(),\n",
    "    T.arange(tgt_inp.shape[1]).dimshuffle('x', 0).repeat(tgt_inp.shape[0], axis=0).flatten(),\n",
    "    tgt_op.flatten()\n",
    "]) * tgt_mask.flatten()).sum() / T.neq(tgt_mask, 0).sum()\n",
    "print cost.eval({tgt_inp:tgt_inp_t, tgt_mask:tgt_mask_t, tgt_op:tgt_op_t, src_inp:src_inp_t, src_lengths:src_lengths_t})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cost = T.nnet.categorical_crossentropy(proj_output_rep, tgt_op).mean()\n",
    "#print cost.eval({tgt_inp:tgt_inp_t, src_inp:src_inp_t, tgt_op:tgt_op_t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#params = src_embedding_layer.params + tgt_embedding_layer.params + src_lstm_forward.params + src_lstm_backward.params + tgt_lstm.params[:-1] + tgt_projection_layer.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates=LearningMethod(clip=5.0).get_updates('adam', cost, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_train = theano.function(\n",
    "    inputs=[src_sentence, tgt_sentence, tgt_gold],\n",
    "    outputs=cost,\n",
    "    updates=updates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_eval = theano.function(\n",
    "    inputs=[src_sentence, tgt_sentence],\n",
    "    outputs=prediction,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(src_sents, tgt_sents, valid=False):\n",
    "    assert len(src_sents) == len(tgt_sents)\n",
    "    src_lengths = [len(sent) for sent in src_sents]\n",
    "    src_max_len = max(src_lengths)\n",
    "    if valid == False:\n",
    "        tgt_lengths = [len(sent) for sent in tgt_sents]\n",
    "        tgt_max_len = max(tgt_lengths)\n",
    "    return (\n",
    "        np.array([[src_word2ind[x] for x in sent] + ([0] * (src_max_len - len(sent))) for sent in src_sents]).astype(np.int32),\n",
    "        np.array(src_lengths).astype(np.int32),\n",
    "        np.array([[target_word2ind[x] for x in sent[:-1]] + ([0] * (tgt_max_len - len(sent))) for sent in tgt_sents]).astype(np.int32),\n",
    "        np.array([[target_word2ind[x] for x in sent[1:]] + ([0] * (tgt_max_len - len(sent))) for sent in tgt_sents]).astype(np.int32),\n",
    "        [([1] * (l - 1)) + ([0] * (tgt_max_len - l)) for l in tgt_lengths]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_validation_predictions():\n",
    "    validation_predictions = []    \n",
    "    for ind, sent in enumerate(dev_src):\n",
    "        \n",
    "        if ind % 300 == 0:\n",
    "            print ind, len(dev_src)\n",
    "        src_words = np.array([src_word2ind[x] for x in sent]).astype(np.int32)\n",
    "        current_outputs = [target_word2ind['<s>']]\n",
    "\n",
    "        while True:\n",
    "            next_word = f_eval(src_words, current_outputs).argmax(axis=1)[-1]\n",
    "            current_outputs.append(next_word)\n",
    "            #print [target_ind2word[x] for x in current_outputs]\n",
    "            if next_word == target_word2ind['</s>'] or len(current_outputs) >= 15:\n",
    "                validation_predictions.append([target_ind2word[x] for x in current_outputs])\n",
    "                break\n",
    "    return validation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_predictions():\n",
    "    test_predictions = []    \n",
    "    for ind, sent in enumerate(test_src):\n",
    "        \n",
    "        if ind % 300 == 0:\n",
    "            print ind, len(test_src)\n",
    "        src_words = np.array([src_word2ind[x] for x in sent]).astype(np.int32)\n",
    "        current_outputs = [target_word2ind['<s>']]\n",
    "\n",
    "        while True:\n",
    "            next_word = f_eval(src_words, current_outputs).argmax(axis=1)[-1]\n",
    "            current_outputs.append(next_word)\n",
    "            #print [target_ind2word[x] for x in current_outputs]\n",
    "            if next_word == target_word2ind['</s>'] or len(current_outputs) >= 15:\n",
    "                test_predictions.append([target_ind2word[x] for x in current_outputs])\n",
    "                break\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 506\n",
      "300 506\n"
     ]
    }
   ],
   "source": [
    "test_preds = get_test_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1006\n",
      "300 1006\n",
      "600 1006\n",
      "900 1006\n"
     ]
    }
   ],
   "source": [
    "validation_preds = get_validation_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'27.43'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_validation_bleu(valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "0 8.04662\n",
      "0 1006\n",
      "300 1006\n",
      "600 1006\n",
      "900 1006\n",
      "===================================================================\n",
      "Epoch 0 BLEU on Validation : 0.00 \n",
      "===================================================================\n",
      "Found new best validation score 0.000000 \n",
      "300 5.16943\n",
      "600 4.57613\n",
      "900 4.38551\n",
      "1200 4.1105\n",
      "1500 3.98464\n",
      "1800 3.92859\n",
      "2100 3.89003\n",
      "2400 3.71789\n",
      "2700 3.63692\n",
      "3000 3.69421\n",
      "3300 3.69565\n",
      "3600 3.57332\n",
      "3900 3.38783\n",
      "4200 3.43899\n",
      "4500 3.35878\n",
      "4800 3.34311\n",
      "5100 3.30067\n",
      "5400 3.27104\n",
      "5700 3.21626\n",
      "6000 3.218\n",
      "6300 3.14793\n",
      "6600 3.12955\n",
      "6900 3.15637\n",
      "7200 3.1899\n",
      "7500 3.0563\n",
      "7800 3.05287\n",
      "8100 3.03736\n",
      "8400 2.87534\n",
      "8700 2.89743\n",
      "9000 2.9673\n",
      "9300 2.68556\n",
      "9600 2.81001\n",
      "9900 2.88605\n",
      "0 1006\n",
      "300 1006\n",
      "600 1006\n",
      "900 1006\n",
      "===================================================================\n",
      "Epoch 0 BLEU on Validation : 17.22 \n",
      "===================================================================\n",
      "Found new best validation score 17.220000 \n",
      "10200 2.76828\n",
      "10500 2.72396\n",
      "10800 2.75616\n",
      "11100 2.81163\n",
      "11400 2.68948\n",
      "11700 2.74052\n",
      "12000 2.72004\n",
      "12300 2.57775\n",
      "12600 2.70213\n",
      "12900 2.66731\n",
      "13200 2.5934\n",
      "13500 2.5938\n",
      "13800 2.58834\n",
      "14100 2.67481\n",
      "14400 2.50966\n",
      "14700 2.63613\n",
      "15000 2.63399\n",
      "15300 2.56521\n",
      "15600 2.39187\n",
      "15900 2.41684\n",
      "16200 2.38342\n",
      "16500 2.45405\n",
      "16800 2.51521\n",
      "17100 2.53305\n",
      "17400 2.31624\n",
      "17700 2.37325\n",
      "18000 2.40507\n",
      "18300 2.4058\n",
      "18600 2.39268\n",
      "18900 2.39772\n",
      "19200 2.43168\n",
      "19500 2.38474\n",
      "19800 2.19672\n",
      "0 1006\n",
      "300 1006\n",
      "600 1006\n",
      "900 1006\n",
      "===================================================================\n",
      "Epoch 0 BLEU on Validation : 20.91 \n",
      "===================================================================\n",
      "Found new best validation score 20.910000 \n",
      "20100 2.41552\n",
      "20400 2.4139\n",
      "20700 2.32563\n",
      "21000 2.33034\n",
      "21300 2.26866\n",
      "21600 2.34584\n",
      "21900 2.1501\n",
      "22200 2.28626\n",
      "22500 2.12523\n",
      "22800 2.21828\n",
      "23100 2.11146\n",
      "23400 2.2092\n",
      "23700 2.22984\n",
      "24000 2.17755\n",
      "24300 2.17103\n",
      "24600 2.14529\n",
      "24900 2.23806\n",
      "25200 2.13203\n",
      "25500 2.24665\n",
      "25800 2.23551\n",
      "26100 2.04\n",
      "26400 2.11538\n",
      "26700 2.19704\n",
      "27000 2.08885\n",
      "27300 2.09719\n",
      "27600 2.10654\n",
      "27900 2.13344\n",
      "28200 2.05091\n",
      "28500 2.07589\n",
      "28800 2.08979\n",
      "29100 2.0183\n",
      "29400 2.17526\n",
      "29700 2.06784\n",
      "30000 2.10568\n",
      "0 1006\n",
      "300 1006\n",
      "600"
     ]
    }
   ],
   "source": [
    "f = open('blue_valid_log.txt', 'w')\n",
    "all_costs = []\n",
    "batch_size = 50\n",
    "n_epochs = 100\n",
    "best_valid_preds = None\n",
    "best_valid_score = -sys.maxint\n",
    "best_test_preds = None\n",
    "for i in xrange(n_epochs):\n",
    "    print 'Starting epoch %i' % i\n",
    "    indices = range(len(train_src))\n",
    "    np.random.shuffle(indices)\n",
    "    train_src_batch = [train_src[ind] for ind in indices]\n",
    "    train_tgt_batch = [train_tgt[ind] for ind in indices]\n",
    "    assert len(train_src_batch) == len(train_tgt_batch)\n",
    "    costs = []\n",
    "    for j in xrange(len(train_src_batch)):\n",
    "        #s_sent, s_length, t_inp, t_op, mask = get_batch(train_src_batch[j:j + batch_size], train_tgt_batch[j:j+batch_size])\n",
    "        new_cost = f_train(\n",
    "            np.array([src_word2ind[x] for x in train_src_batch[j]]).astype(np.int32),\n",
    "            np.array([target_word2ind[x] for x in train_tgt_batch[j]][:-1]).astype(np.int32),\n",
    "            np.array([target_word2ind[x] for x in train_tgt_batch[j]][1:]).astype(np.int32),\n",
    "        )\n",
    "        all_costs.append((j, new_cost))\n",
    "        costs.append(new_cost)\n",
    "        if j % 300 == 0:\n",
    "            print j, np.mean(costs)\n",
    "            costs = []\n",
    "        if np.isnan(new_cost):\n",
    "            print 'NaN detected.'\n",
    "            break\n",
    "        if j % 10000 == 0:\n",
    "            valid_preds = get_validation_predictions()\n",
    "            print '==================================================================='\n",
    "            print 'Epoch %i BLEU on Validation : %s ' % (i, get_validation_bleu(valid_preds))\n",
    "            print '==================================================================='\n",
    "            if float(get_validation_bleu(valid_preds)) >= best_valid_score:\n",
    "                best_valid_score = float(get_validation_bleu(valid_preds))\n",
    "                best_valid_preds = copy.deepcopy(valid_preds)\n",
    "                best_test_preds = copy.deepcopy(get_test_predictions())\n",
    "                print 'Found new best validation score %f ' % (best_valid_score)\n",
    "            f.write('Epoch %d Minibatch %d BLEU on Validation : %s \\n' % (i, j, get_validation_bleu(valid_preds)))\n",
    "\n",
    "    if np.isnan(new_cost):\n",
    "        print 'NaN detected.'\n",
    "        break\n",
    "    valid_preds = get_validation_predictions()\n",
    "    print '==================================================================='\n",
    "    print 'Epoch %i BLEU on Validation : %s ' % (i, get_validation_bleu(valid_preds))\n",
    "    print '==================================================================='\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('output.txt', 'w')\n",
    "for line in test_preds:\n",
    "    f.write(' '.join(line) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'28.21'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_validation_bleu(valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu_stats(hypothesis, reference):\n",
    "    stats = []\n",
    "    stats.append(len(hypothesis))\n",
    "    stats.append(len(reference))\n",
    "    for n in xrange(1,5):\n",
    "        s_ngrams = Counter([tuple(hypothesis[i:i+n]) for i in xrange(len(hypothesis)+1-n)])\n",
    "        r_ngrams = Counter([tuple(reference[i:i+n]) for i in xrange(len(reference)+1-n)])\n",
    "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
    "        stats.append(max([len(hypothesis)+1-n, 0]))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu(stats):\n",
    "    if len(filter(lambda x: x==0, stats)) > 0:\n",
    "        return 0\n",
    "    (c, r) = stats[:2]\n",
    "    log_bleu_prec = sum([math.log(float(x)/y) for x,y in zip(stats[2::2],stats[3::2])]) / 4.\n",
    "    return math.exp(min([0, 1-float(r)/c]) + log_bleu_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_bleu(hypotheses):\n",
    "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    ref_lines = [line.strip().split() for line in open(path_to_dev_tgt, 'r')]\n",
    "    for hyp, ref in zip(hypotheses, ref_lines):\n",
    "        stats += np.array(bleu_stats(hyp, ref))\n",
    "    return \"%.2f\" % (100*bleu(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
